{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate application using manual data set\n",
    "\n",
    "## Objective\n",
    "\n",
    "This tutorial provides a step-by-step guide on how to use the simulator to automate interaction with the LLM. This will allow user to create synthetic test data. After data has been simulated we use the evaluation SDK to score the output/\n",
    "\n",
    "This tutorial uses the following Azure AI services:\n",
    "\n",
    "- [azure-ai-evaluation](https://learn.microsoft.com/en-us/azure/ai-studio/how-to/develop/evaluate-sdk)\n",
    "\n",
    "## Before you begin\n",
    "\n",
    "### Installation\n",
    "\n",
    "Install the following packages required to execute this notebook. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install azure-ai-evaluation\n",
    "%pip install promptflow-azure\n",
    "%pip install azure-identity\n",
    "%pip install --upgrade openai\n",
    "%pip install python-dotenv\n",
    "#%pip install marshmallow==3.23.3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters and imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import json\n",
    "import importlib.resources as pkg_resources\n",
    "import requests\n",
    "from typing import Any, Dict, List, Optional\n",
    "from pathlib import Path\n",
    "from pprint import pprint\n",
    "from azure.ai.evaluation import evaluate\n",
    "from azure.ai.evaluation import GroundednessEvaluator\n",
    "from azure.ai.evaluation.simulator import Simulator\n",
    "from openai import AzureOpenAI\n",
    "from azure.identity import DefaultAzureCredential, get_bearer_token_provider\n",
    "from dotenv import load_dotenv\n",
    "from application_endpoint import ApplicationEndpoint\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We set up the configuration for AI Foundry Projet and model config. This will be use for result visibility and use LLM as the judge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "project_scope = {\n",
    "    \"subscription_id\": os.environ.get(\"AZURE_SUBSCRIPTION_ID\"),\n",
    "    \"resource_group_name\": os.environ.get(\"AZURE_AI_FOUNDRY_RESOURCE_GROUP\"),\n",
    "    \"project_name\": os.environ.get(\"AZURE_AI_FOUNDRY_PROJECT_NAME\"),\n",
    "}\n",
    "model_config = {\n",
    "    \"azure_endpoint\": os.environ.get(\"AZURE_OPENAI_ENDPOINT\"),\n",
    "    \"azure_deployment\": os.environ.get(\"AZURE_OPENAI_DEPLOYMENT\"),\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AI Foundry SDK provide the similuator to be able to simulate the interaction with the LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simulator = Simulator(model_config=model_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Connect application end point to the simulator. The call back function map the input and output between application endpoint and the suitable format for the evaluator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def callback(\n",
    "    messages: List[Dict],\n",
    "    stream: bool = False,\n",
    "    session_state: Any = None,  # noqa: ANN401\n",
    "    context: Optional[Dict[str, Any]] = None,\n",
    ") -> dict:\n",
    "    messages_list = messages[\"messages\"]\n",
    "    # get last message\n",
    "    latest_message = messages_list[-1]\n",
    "    query = latest_message[\"content\"]\n",
    "    context = latest_message.get(\"context\", None)\n",
    "    # call model end point\n",
    "    application_endpoint = ApplicationEndpoint()\n",
    "    response = application_endpoint(query, None)\n",
    "    print(response)\n",
    "    # we are formatting the response to follow the openAI chat protocol format\n",
    "    formatted_response = {\n",
    "        \"content\": response[\"response\"],\n",
    "        \"role\": \"assistant\",\n",
    "        \"context\": context,\n",
    "    }\n",
    "    messages[\"messages\"].append(formatted_response)\n",
    "    return {\"messages\": messages[\"messages\"], \"stream\": stream, \"session_state\": session_state, \"context\": context}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We trigger the simular and provide some configuration where we will ask the simular to try to complete two task with 2 turn of the conversation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = await simulator(\n",
    "    target=callback,\n",
    "    num_queries=2,\n",
    "    max_conversation_turns=2,\n",
    "    tasks=[\n",
    "        f\"I want to learn more about Responsible AI\",\n",
    "        f\"I want to know how to implement Responsible AI in my organization\",\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After simulation has been completed we can have a write a output to a file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simulated_output_file = Path(\"simulated_output.json\")\n",
    "with simulated_output_file.open(\"a\") as f:\n",
    "    json.dump(outputs, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The simulated response include the full conversation of the simulator with the application. In order to evaluate the output we need to convert to the standard format using to_eval_qr_json_lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_data_file = Path(\"simulated_eval_data.jsonl\")\n",
    "with eval_data_file.open(\"w\") as file:\n",
    "    for output in outputs:\n",
    "        file.write(output.to_eval_qr_json_lines())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Please provide Azure AI Project details so that traces and eval results are pushing in the project in Azure AI Studio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "azure_ai_project = {\n",
    "    \"subscription_id\": os.environ[\"AZURE_SUBSCRIPTION_ID\"],\n",
    "    \"resource_group_name\": os.environ[\"AZURE_AI_FOUNDRY_RESOURCE_GROUP\"],\n",
    "    \"project_name\": os.environ[\"AZURE_AI_FOUNDRY_PROJECT_NAME\"],\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data\n",
    "\n",
    "Following code reads Json file \"data.jsonl\" which contains inputs to the Application Target function. It provides question, context on each line. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_json(\"simulated_eval_data.jsonl\", lines=True)\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To use Relevance and Cohenrence Evaluator, we will Azure Open AI model details as a Judge that can be passed as model config."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "model_config = {\n",
    "    \"azure_endpoint\": os.environ.get(\"AZURE_OPENAI_ENDPOINT\"),\n",
    "    \"azure_deployment\": os.environ.get(\"AZURE_OPENAI_DEPLOYMENT\"),\n",
    "    \"api_key\": os.environ.get(\"AZURE_OPENAI_KEY\"),\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the evaluation\n",
    "\n",
    "The Following code runs Evaluate API and uses Content Safety, Relevance and Coherence Evaluator to evaluate results from different models.\n",
    "\n",
    "The following are the few parameters required by Evaluate API. \n",
    "\n",
    "+   Data file (Prompts): It represents data file 'simulated_eval_data.jsonl' in JSON format. Each line contains question, context for evaluators.     \n",
    "\n",
    "+   Evaluators: List of evaluators is provided, to evaluate given prompts (questions) as input and output (answers) from LLM models. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "from datetime import datetime\n",
    "from azure.ai.evaluation import evaluate\n",
    "from azure.ai.evaluation import (\n",
    "    ContentSafetyEvaluator,\n",
    "    RelevanceEvaluator,\n",
    "    CoherenceEvaluator,\n",
    "    GroundednessEvaluator,\n",
    "    FluencyEvaluator,\n",
    "    SimilarityEvaluator,\n",
    ")\n",
    "from application_endpoint import ApplicationEndpoint\n",
    "\n",
    "\n",
    "content_safety_evaluator = ContentSafetyEvaluator(\n",
    "    azure_ai_project=azure_ai_project, credential=DefaultAzureCredential()\n",
    ")\n",
    "relevance_evaluator = RelevanceEvaluator(model_config)\n",
    "coherence_evaluator = CoherenceEvaluator(model_config)\n",
    "groundedness_evaluator = GroundednessEvaluator(model_config)\n",
    "fluency_evaluator = FluencyEvaluator(model_config)\n",
    "similarity_evaluator = SimilarityEvaluator(model_config)\n",
    "\n",
    "path = str(pathlib.Path(pathlib.Path.cwd())) + \"/simulated_eval_data.jsonl\"\n",
    "\n",
    "results = evaluate(\n",
    "    evaluation_name=f\"Simulated-Eval-Run-{datetime.now().strftime(\"%Y-%m-%d\")}\",\n",
    "    data=path,\n",
    "    evaluators={\n",
    "        \"content_safety\": content_safety_evaluator,\n",
    "        \"coherence\": coherence_evaluator,\n",
    "        \"relevance\": relevance_evaluator,\n",
    "        \"groundedness\": groundedness_evaluator,\n",
    "        \"fluency\": fluency_evaluator,\n",
    "        # \"similarity\": similarity_evaluator,\n",
    "    },\n",
    "    azure_ai_project=azure_ai_project,\n",
    "    evaluator_config={\n",
    "        \"content_safety\": {\"column_mapping\": {\"query\": \"${data.query}\", \"response\": \"${data.response}\"}},\n",
    "        \"coherence\": {\"column_mapping\": {\"response\": \"${data.response}\", \"query\": \"${data.query}\"}},\n",
    "        \"relevance\": {\n",
    "            \"column_mapping\": {\"response\": \"${data.response}\", \"context\": \"${data.context}\", \"query\": \"${data.query}\"}\n",
    "        },\n",
    "        \"groundedness\": {\n",
    "            \"column_mapping\": {\n",
    "                \"response\": \"${data.response}\",\n",
    "                \"context\": \"${data.context}\",\n",
    "                \"query\": \"${data.query}\",\n",
    "            }\n",
    "        },\n",
    "        \"fluency\": {\n",
    "            \"column_mapping\": {\"response\": \"${data.response}\", \"context\": \"${data.context}\", \"query\": \"${data.query}\"}\n",
    "        },\n",
    "        # \"similarity\": {\n",
    "        #     \"column_mapping\": {\"response\": \"${data.response}\", \"context\": \"${data.context}\", \"query\": \"${data.query}\"}\n",
    "        # },\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "View the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(results[\"rows\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
